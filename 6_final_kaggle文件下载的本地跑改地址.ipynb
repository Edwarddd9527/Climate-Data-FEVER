{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8270398,"sourceType":"datasetVersion","datasetId":4910213},{"sourceId":8388833,"sourceType":"datasetVersion","datasetId":4962799}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nnltk.download('punkt')\n\n# 读取声明和证据数据\nwith open('/kaggle/input/train-dataset/train-claims.json', 'r') as file:\n    data = json.load(file)\nwith open('/kaggle/input/train-dataset/evidence.json', 'r') as file:\n    evidence_data = json.load(file)\n\n# 准备输出数据列表\noutput_data = []\nfor claim_id, claim_info in data.items():\n    claim_text = claim_info['claim_text']\n    claim_label = claim_info['claim_label']\n    if claim_label == \"DISPUTED\":\n        continue  # 忽略 disputed 的声明\n    label_mapping = {\n        \"SUPPORTS\": \"support\",\n        \"REFUTES\": \"refute\",\n        \"NOT_ENOUGH_INFO\": \"irrelevant\"\n    }\n    if claim_label in label_mapping:\n        for evidence_id in claim_info['evidences']:\n            output_data.append({\n                \"claim_id\": claim_id,\n                \"claim_text\": claim_text,\n                \"evidence_id\": evidence_id,\n                \"label\": label_mapping[claim_label],\n                \"evidence_text\": evidence_data.get(evidence_id, \"\")\n            })\n\n# 转换为 DataFrame 并分割数据集\ndf_output = pd.DataFrame(output_data)\ntrain_df, valid_df = train_test_split(df_output, test_size=0.2, random_state=42)\n\n# 函数来预处理和向量化文本\ndef preprocess_text(text):\n    pattern = re.compile(r'\\b[a-zA-Z0-9]+\\b')\n    tokens = word_tokenize(text.lower())\n    filtered_tokens = [token for token in tokens if pattern.match(token)]\n    return filtered_tokens\n\n# 准备文档\ntrain_documents = [TaggedDocument(words=preprocess_text(row['claim_text']) + preprocess_text(row['evidence_text']), tags=[str(i)]) for i, row in train_df.iterrows()]\nvalid_documents = [TaggedDocument(words=preprocess_text(row['claim_text']) + preprocess_text(row['evidence_text']), tags=[str(i)]) for i, row in valid_df.iterrows()]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-12T06:03:40.107807Z","iopub.execute_input":"2024-05-12T06:03:40.108338Z","iopub.status.idle":"2024-05-12T06:03:55.968463Z","shell.execute_reply.started":"2024-05-12T06:03:40.108302Z","shell.execute_reply":"2024-05-12T06:03:55.967637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 初始化 Doc2Vec 模型\nmodel_d2v = Doc2Vec(vector_size=200, window=5, min_count=1, workers=4, epochs=50)\n\n# 构建词汇表并训练模型\nmodel_d2v.build_vocab(train_documents)\nmodel_d2v.train(train_documents, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:03:55.969620Z","iopub.execute_input":"2024-05-12T06:03:55.970234Z","iopub.status.idle":"2024-05-12T06:04:20.906703Z","shell.execute_reply.started":"2024-05-12T06:03:55.970180Z","shell.execute_reply":"2024-05-12T06:04:20.905692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 设置计算设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef get_vectors(documents):\n    vectors = np.array([model_d2v.infer_vector(doc.words) for doc in documents])\n    return torch.tensor(vectors, dtype=torch.float).view(vectors.shape[0], 1, -1).to(device)\n\n# 获取训练和验证数据向量\ntrain_vectors = get_vectors(train_documents)\nvalid_vectors = get_vectors(valid_documents)\n\n# 获取标签并转换为 tensor\ntrain_labels = torch.tensor(train_df['label'].astype('category').cat.codes.to_numpy(), dtype=torch.long).to(device)\nvalid_labels = torch.tensor(valid_df['label'].astype('category').cat.codes.to_numpy(), dtype=torch.long).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:04:20.907979Z","iopub.execute_input":"2024-05-12T06:04:20.908306Z","iopub.status.idle":"2024-05-12T06:04:43.554077Z","shell.execute_reply.started":"2024-05-12T06:04:20.908278Z","shell.execute_reply":"2024-05-12T06:04:43.553084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# LSTM 模型定义\nclass LSTMModel(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,\n                            bidirectional=True, dropout=0.5, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n\n    def forward(self, x):\n        _, (hidden, _) = self.lstm(x)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        return self.fc(hidden)\n\n\n# 实例化模型\nmodel = LSTMModel(200, 256, 3).to(device)\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n    \n# 训练函数\ndef train(model, train_vectors, train_labels, valid_vectors, valid_labels, optimizer, criterion, n_epochs, patience):\n    min_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(n_epochs):\n        model.train()  # 确保模型处于训练模式\n        optimizer.zero_grad()\n        outputs = model(train_vectors)\n        loss = criterion(outputs, train_labels)\n        loss.backward()\n        optimizer.step()\n        \n        # 评估模式，计算验证损失和准确率\n        model.eval()\n        valid_loss, valid_accuracy = evaluate(model, valid_vectors, valid_labels)\n        \n        # 打印每个epoch的训练信息\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Valid Loss: {valid_loss}, Valid Acc: {valid_accuracy:.2f}')\n        \n        # 早停判断\n        if valid_loss < min_loss:\n            min_loss = valid_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= patience:\n            print(\"Stopping early due to lack of improvement.\")\n            break\n\n        model.train()  # 确保在下一个epoch开始之前，模型回到训练模式\n\n\ndef evaluate(model, valid_vectors, valid_labels):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(valid_vectors)\n        loss = criterion(outputs, valid_labels)\n        _, predicted = torch.max(outputs, 1)\n        correct = (predicted == valid_labels).sum().item()\n        accuracy = correct / valid_labels.size(0)\n    return loss.item(), accuracy\n\n\n\n# 执行训练\nn_epochs = 800\ntrain(model, train_vectors, train_labels, valid_vectors, valid_labels,\n      optimizer, criterion, n_epochs=400, patience=200)\n\n# 执行评估\n# 正确提取准确率进行打印\nvalidation_loss, validation_accuracy = evaluate(model, valid_vectors, valid_labels)\nprint(f'Validation Accuracy: {validation_accuracy:.2f}')\n ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:04:43.556617Z","iopub.execute_input":"2024-05-12T06:04:43.556909Z","iopub.status.idle":"2024-05-12T06:04:48.018513Z","shell.execute_reply.started":"2024-05-12T06:04:43.556884Z","shell.execute_reply":"2024-05-12T06:04:48.017564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 保存模型\ntorch.save(model.state_dict(), 'lstm_model.pth')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 打印混淆矩阵\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(valid_vectors)\n    _, predicted = torch.max(outputs, 1)\n    cm = confusion_matrix(valid_labels.cpu(), predicted.cpu())\n    sns.heatmap(cm, annot=True, fmt='d', xticklabels=['support', 'refute', 'irrelevant'], yticklabels=['support', 'refute', 'irrelevant'])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:04:48.019947Z","iopub.execute_input":"2024-05-12T06:04:48.020652Z","iopub.status.idle":"2024-05-12T06:04:48.286623Z","shell.execute_reply.started":"2024-05-12T06:04:48.020616Z","shell.execute_reply":"2024-05-12T06:04:48.285697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\n# 指定 NLTK 使用已上传的数据文件\nnltk.data.path.append('/kaggle/input/nlp111/nltk_data')  # 确保路径是正确的\n\n# 测试 WordNet 是否正确加载\nfrom nltk.corpus import wordnet\ntry:\n    # 测试查找一个词的同义词集合以确认数据被正确加载\n    synsets = wordnet.synsets('dog')\n    print(\"WordNet加载成功，找到'狗'的同义词集：\", synsets)\nexcept Exception as e:\n    print(\"加载WordNet数据时发生错误：\", e)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:04:48.287758Z","iopub.execute_input":"2024-05-12T06:04:48.288031Z","iopub.status.idle":"2024-05-12T06:04:50.510716Z","shell.execute_reply.started":"2024-05-12T06:04:48.288008Z","shell.execute_reply":"2024-05-12T06:04:50.509661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\n\n# 将下载目录添加到 nltk 的搜索路径\nnltk.data.path.append('/kaggle/input/nlp111/nltk_data')\n\n# 测试是否可以正确加载 WordNet\nfrom nltk.corpus import wordnet\nprint(wordnet.synsets('dog'))  # 试着加载一个示例以确认 WordNet 可用\n\n\ndef synonym_replacement(text, num_replacements):\n    \"\"\"用同义词替换文本中的词来增强数据\"\"\"\n    words = nltk.word_tokenize(text)\n    new_text = words[:]\n    random_word_list = list(set([word for word in words if word.isalpha()]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for word in random_word_list:\n        synonyms = get_synonyms(word)\n        if len(synonyms) > 0:\n            synonym = random.choice(list(synonyms))\n            new_text = [synonym if w == word else w for w in new_text]\n            num_replaced += 1\n        if num_replaced >= num_replacements:  # 控制替换数量\n            break\n\n    return ' '.join(new_text)\n\ndef get_synonyms(word):\n    \"\"\"获取单词的同义词集合\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for l in syn.lemmas():\n            synonym = l.name().replace('_', ' ').replace('-', ' ').lower()\n            synonyms.add(synonym)\n    if word in synonyms:\n        synonyms.remove(word)\n    return synonyms\n\n# 接下来修改数据读取和预处理的部分，增加数据增强\n\nimport pandas as pd\nimport numpy as np\n\ndef augment_data(data, augment_rate=1):\n    \"\"\"根据给定的增强率增强数据\"\"\"\n    new_rows = []\n    for _, row in data.iterrows():\n        new_rows.append(row)  # 添加原始行\n        if row['claim_label'] in ['DISPUTED', 'REFUTES']:\n            additions = 2  # 增加200%\n        else:\n            additions = 1  # 增加100%\n        for _ in range(additions):\n            new_row = row.copy()\n            new_row['claim_text'] = synonym_replacement(row['claim_text'], num_replacements=2)\n            new_rows.append(new_row)\n    return pd.DataFrame(new_rows)\n\n# 读取和增强数据\ntrain_data = pd.read_csv('/kaggle/input/nlp111/training_data_top_50_evidences.csv')\ndev_data = pd.read_csv('/kaggle/input/nlp111/dev_data_top_50_evidences.csv')\n\ntrain_data = augment_data(train_data)\ndev_data = augment_data(dev_data)  # 可选择是否对开发集进行数据增强\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:04:50.511867Z","iopub.execute_input":"2024-05-12T06:04:50.512164Z","iopub.status.idle":"2024-05-12T06:05:46.793858Z","shell.execute_reply.started":"2024-05-12T06:04:50.512138Z","shell.execute_reply":"2024-05-12T06:05:46.792822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 后续步骤（训练和验证模型）保持不变\n\n# 继续使用之前定义的Doc2Vec模型和LSTM模型\n# 假设这些模型都已经被定义和训练过了，并且已经加载到当前的环境中\n\ndef preprocess_and_vectorize(text):\n    \"\"\"文本预处理并返回向量化的结果\"\"\"\n    tokens = preprocess_text(text)  # 使用前面定义的preprocess_text函数\n    vector = model_d2v.infer_vector(tokens)  # 使用Doc2Vec模型生成向量\n    return vector\n\ndef predict_evidence(model, evidence_vector):\n    \"\"\"预测单个证据的标签\"\"\"\n    vector_tensor = torch.tensor(evidence_vector, dtype=torch.float).view(1, 1, -1).to(device)\n    model.eval()\n    with torch.no_grad():\n        output = model(vector_tensor)\n        _, predicted = torch.max(output, 1)\n    return predicted.item()  # 返回预测的标签索引\n\ndef feature_engineering(predictions):\n    \"\"\"基于预测结果生成特征\"\"\"\n    counts = np.bincount(predictions, minlength=3)\n    return counts / np.sum(counts)  # 返回每个标签的相对频率作为特征\n\n# 处理claims并生成训练特征和标签\ndef process_claims(data, model):\n    features = []\n    labels = []\n    grouped = data.groupby('claim_id')\n    for claim_id, group in grouped:\n        predictions = []\n        for _, row in group.iterrows():\n            vector = preprocess_and_vectorize(row['evidence_text'])\n            pred = predict_evidence(model, vector)\n            predictions.append(pred)\n        claim_features = feature_engineering(predictions)\n        features.append(claim_features)\n        labels.append(row['claim_label'])  # 假设每个group的最后一行包含claim的标签\n    return np.array(features), np.array(labels)\n\n# 生成训练和验证特征\ntrain_features, train_labels = process_claims(train_data, model)\nvalid_features, valid_labels = process_claims(dev_data, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:05:46.795295Z","iopub.execute_input":"2024-05-12T06:05:46.795598Z","iopub.status.idle":"2024-05-12T06:20:14.829695Z","shell.execute_reply.started":"2024-05-12T06:05:46.795573Z","shell.execute_reply":"2024-05-12T06:20:14.828836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练逻辑回归模型\nlr_model = LogisticRegression(random_state=42)\nlr_model.fit(train_features, train_labels)\n\n# 验证模型\nvalid_predictions = lr_model.predict(valid_features)\naccuracy = accuracy_score(valid_labels, valid_predictions)\nprint(f'Validation Accuracy: {accuracy:.2f}')\n\n# 绘制混淆矩阵\ncm = confusion_matrix(valid_labels, valid_predictions)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=lr_model.classes_, yticklabels=lr_model.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:20:14.830868Z","iopub.execute_input":"2024-05-12T06:20:14.831160Z","iopub.status.idle":"2024-05-12T06:20:15.159082Z","shell.execute_reply.started":"2024-05-12T06:20:14.831135Z","shell.execute_reply":"2024-05-12T06:20:15.158030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(valid_labels, valid_predictions, target_names=lr_model.classes_))\nprint(lr_model.coef_)\nprint(len(valid_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:20:15.160710Z","iopub.execute_input":"2024-05-12T06:20:15.161062Z","iopub.status.idle":"2024-05-12T06:20:15.176944Z","shell.execute_reply.started":"2024-05-12T06:20:15.161031Z","shell.execute_reply":"2024-05-12T06:20:15.176054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 加载数据，这里假设你已经有了处理好的特征和标签\n# train_features, train_labels\n# valid_features, valid_labels\n\n# 定义基模型\nbase_models = [\n    ('lr', LogisticRegression(random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n    ('svm', SVC(kernel='linear', probability=True, random_state=42)),\n    ('mlp', MLPClassifier(hidden_layer_sizes=(50,), random_state=42)),\n    ('gnb', GaussianNB()),\n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n]\n\n# 定义元学习器\nmeta_learner = LGBMClassifier(random_state=42)\n\n# 创建堆叠分类器\nstack = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5, stack_method='predict_proba')\n\n# 训练模型\nstack.fit(train_features, train_labels)\n\n# 验证模型\nvalid_predictions = stack.predict(valid_features)\naccuracy = accuracy_score(valid_labels, valid_predictions)\nprint(f'Validation Accuracy: {accuracy:.2f}')\n\n# 混淆矩阵和分类报告\nprint(\"\\nClassification Report:\\n\", classification_report(valid_labels, valid_predictions))\ncm = confusion_matrix(valid_labels, valid_predictions)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=stack.classes_, yticklabels=stack.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:20:15.178119Z","iopub.execute_input":"2024-05-12T06:20:15.178490Z","iopub.status.idle":"2024-05-12T06:20:23.484594Z","shell.execute_reply.started":"2024-05-12T06:20:15.178466Z","shell.execute_reply":"2024-05-12T06:20:23.483630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 定义基模型，注意有些模型需要数据预处理，比如归一化\nbase_models = [\n    ('lr', LogisticRegression(random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n    ('svm', make_pipeline(StandardScaler(), LinearSVC(random_state=42))),\n    ('mlp', MLPClassifier(hidden_layer_sizes=(50,), random_state=42)),\n    ('gnb', MultinomialNB()),\n    ('sgd', make_pipeline(StandardScaler(), SGDClassifier(random_state=42))),\n    ('lgbm', LGBMClassifier(random_state=42)),\n    ('ridge', RidgeClassifier(random_state=42)),\n    ('perceptron', make_pipeline(StandardScaler(), Perceptron(random_state=42))),\n    ('pac', make_pipeline(StandardScaler(), PassiveAggressiveClassifier(random_state=42))),\n    ('gbc', GradientBoostingClassifier(random_state=42)),\n    ('etc', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n    ('knn', make_pipeline(StandardScaler(), KNeighborsClassifier()))\n]\n\n# 定义元学习器\nmeta_learner = LGBMClassifier(random_state=42)\n\n# 创建堆叠分类器\nstack = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5, stack_method='auto')\n\n# 训练模型\nstack.fit(train_features, train_labels)\n\n# 验证模型\nvalid_predictions = stack.predict(valid_features)\naccuracy = accuracy_score(valid_labels, valid_predictions)\nprint(f'Validation Accuracy: {accuracy:.2f}')\n\n# 混淆矩阵和分类报告\nprint(\"\\nClassification Report:\\n\", classification_report(valid_labels, valid_predictions))\ncm = confusion_matrix(valid_labels, valid_predictions)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=stack.classes_, yticklabels=stack.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:20:23.486013Z","iopub.execute_input":"2024-05-12T06:20:23.486334Z","iopub.status.idle":"2024-05-12T06:20:47.352336Z","shell.execute_reply.started":"2024-05-12T06:20:23.486307Z","shell.execute_reply":"2024-05-12T06:20:47.351352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#保存模型\nimport joblib\njoblib.dump(stack, 'stacking_model.joblib')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T06:31:12.142047Z","iopub.execute_input":"2024-05-12T06:31:12.143099Z","iopub.status.idle":"2024-05-12T06:31:12.436453Z","shell.execute_reply.started":"2024-05-12T06:31:12.143060Z","shell.execute_reply":"2024-05-12T06:31:12.435432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef preprocess_and_vectorize(text):\n    \"\"\"文本预处理并返回向量化的结果\"\"\"\n    tokens = preprocess_text(text)  # 使用前面定义的preprocess_text函数\n    vector = model_d2v.infer_vector(tokens)  # 使用Doc2Vec模型生成向量\n    return vector\n\ndef predict_evidence(model, evidence_vector):\n    \"\"\"预测单个证据的标签\"\"\"\n    vector_tensor = torch.tensor(evidence_vector, dtype=torch.float).view(1, 1, -1).to(device)\n    model.eval()\n    with torch.no_grad():\n        output = model(vector_tensor)\n        _, predicted = torch.max(output, 1)\n    return predicted.item()  # 返回预测的标签索引\n\ndef feature_engineering(predictions):\n    \"\"\"基于预测结果生成特征\"\"\"\n    counts = np.bincount(predictions, minlength=3)\n    return counts / np.sum(counts)  # 返回每个标签的相对频率作为特征\n\n# 处理claims并生成训练特征和标签\ndef process_claims(data, model):\n    features = []\n    labels = []\n    grouped = data.groupby('claim_id')\n    for claim_id, group in grouped:\n        predictions = []\n        for _, row in group.iterrows():\n            vector = preprocess_and_vectorize(row['evidence_text'])\n            pred = predict_evidence(model, vector)\n            predictions.append(pred)\n        claim_features = feature_engineering(predictions)\n        features.append(claim_features)\n        labels.append(row['claim_label'])  # 假设每个group的最后一行包含claim的标签\n    return np.array(features), np.array(labels)\n\ntest_data = pd.read_csv('/kaggle/input/nlp111/test_data_top_50_evidences.csv')\n# 将test_data的label列改名为claim_label\ntest_data.rename(columns={'label': 'claim_label'}, inplace=True)\n\ntest_features, test_labels = process_claims(test_data, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:26:46.777844Z","iopub.execute_input":"2024-05-12T07:26:46.778305Z","iopub.status.idle":"2024-05-12T07:27:33.089108Z","shell.execute_reply.started":"2024-05-12T07:26:46.778269Z","shell.execute_reply":"2024-05-12T07:27:33.088244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = stack.predict(test_features)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:46:01.051141Z","iopub.execute_input":"2024-05-12T07:46:01.051545Z","iopub.status.idle":"2024-05-12T07:46:01.099066Z","shell.execute_reply.started":"2024-05-12T07:46:01.051515Z","shell.execute_reply":"2024-05-12T07:46:01.097868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:36:25.793362Z","iopub.execute_input":"2024-05-12T07:36:25.794169Z","iopub.status.idle":"2024-05-12T07:36:25.799560Z","shell.execute_reply.started":"2024-05-12T07:36:25.794136Z","shell.execute_reply":"2024-05-12T07:36:25.798635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# 假设 test_data 是你的测试数据集 DataFrame，加载方式类似于之前的加载方法\n# 假设 test_predictions 包含按顺序的预测结果，如你所提供的列表\n\ndef format_output(data, predictions, num_evidences):\n    results = {}\n    grouped = data.groupby('claim_id')\n    i = 0  # 使用索引来跟踪predictions中的位置\n    for claim_id, group in grouped:\n        claim_text = group.iloc[0]['claim_text']\n        evidence_ids = group['evidence_id'].tolist()[:num_evidences]\n        predicted_label = predictions[i]  # 获取对应索引的预测结果\n        i += 1  # 移动到下一个claim的预测结果\n        results[claim_id] = {\n            \"claim_text\": claim_text,\n            \"claim_label\": predicted_label,\n            \"evidences\": evidence_ids\n        }\n    return results\n\n# 格式化输出为JSON\nresults_3 = format_output(test_data, test_predictions, 3)\nresults_4 = format_output(test_data, test_predictions, 4)\nresults_5 = format_output(test_data, test_predictions, 5)\n\n# 保存结果为JSON文件\ndef save_results(results, filename):\n    with open(filename, 'w') as file:\n        json.dump(results, file, indent=4)\n\nsave_results(results_3, 'predictions_3.json')\nsave_results(results_4, 'predictions_4.json')\nsave_results(results_5, 'predictions_5.json')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:37:13.284245Z","iopub.execute_input":"2024-05-12T07:37:13.285111Z","iopub.status.idle":"2024-05-12T07:37:13.372376Z","shell.execute_reply.started":"2024-05-12T07:37:13.285047Z","shell.execute_reply":"2024-05-12T07:37:13.371325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\n\n# 加载未处理的claim文本\nwith open('/kaggle/input/train-dataset/test-claims-unlabelled.json', 'r') as file:\n    original_claims = json.load(file)\n\n# 函数用于加载之前保存的预测结果\ndef load_predictions(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\n# 加载之前的预测结果\nresults_3 = load_predictions('/kaggle/working/predictions_3.json')\nresults_4 = load_predictions('/kaggle/working/predictions_4.json')\nresults_5 = load_predictions('/kaggle/working/predictions_5.json')\n\n# 函数用于合并原始文本和预测结果\ndef merge_results(original_claims, predictions):\n    final_results = {}\n    for claim_id, claim_text in original_claims.items():\n        if claim_id in predictions:\n            final_results[claim_id] = {\n                \"claim_text\": claim_text,\n                \"claim_label\": predictions[claim_id]['claim_label'],\n                \"evidences\": predictions[claim_id]['evidences']\n            }\n    return final_results\n\n# 合并结果并保存为新的JSON文件\ndef save_results(results, filename):\n    with open(filename, 'w') as file:\n        json.dump(results, file, indent=4)\n\n# 合并和保存结果\nfinal_results_3 = merge_results(original_claims, results_3)\nfinal_results_4 = merge_results(original_claims, results_4)\nfinal_results_5 = merge_results(original_claims, results_5)\n\nsave_results(final_results_3, '/kaggle/working/predictions_3.json')\nsave_results(final_results_4, '/kaggle/working/predictions_4.json')\nsave_results(final_results_5, '/kaggle/working/predictions_5.json')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T07:53:18.843992Z","iopub.execute_input":"2024-05-12T07:53:18.844623Z","iopub.status.idle":"2024-05-12T07:53:18.876615Z","shell.execute_reply.started":"2024-05-12T07:53:18.844580Z","shell.execute_reply":"2024-05-12T07:53:18.875333Z"},"trusted":true},"execution_count":null,"outputs":[]}]}