{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载词到索引的映射\n",
    "def load_word2idx(file_path):\n",
    "    df = pd.read_csv(file_path, header=None, names=['word', 'index'])\n",
    "    return df.set_index('word')['index'].to_dict()\n",
    "\n",
    "# 加载数据\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# 准备文本数据，使用索引\n",
    "def prepare_indexed_text(data, word2idx):\n",
    "    indexed_data = []\n",
    "    for item in data.values():\n",
    "        words = item['claim_text'].split() if 'claim_text' in item else item.split()\n",
    "        indexed_data.append([word2idx.get(word, word2idx.get('<UNK>', 4)) for word in words])  # 使用<UNK>对未知词汇编码\n",
    "    return indexed_data\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "def train_word2vec(sentences, vector_size=100, window=5):\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=1, workers=4, sg=1)  # 使用skip-gram模型\n",
    "    return model\n",
    "\n",
    "# 文本转向量\n",
    "def text_to_vector(text_indices, model):\n",
    "    return np.mean([model.wv[index] for index in text_indices if index in model.wv.key_to_index], axis=0, where=[True for _ in text_indices if _ in model.wv.key_to_index])\n",
    "\n",
    "word2idx = load_word2idx('data/preprocessing_result.csv')\n",
    "train_claims = load_json('data/train-claims.json')\n",
    "evidences = load_json('data/evidence.json')\n",
    "\n",
    "print('word2idx:', word2idx3)\n",
    "\n",
    "\n",
    "# 准备索引文本数据\n",
    "indexed_claims = prepare_indexed_text(train_claims, word2idx)\n",
    "indexed_evidences = {id: prepare_indexed_text({id: text}, word2idx)[0] for id, text in evidences.items()}\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "all_texts = indexed_claims + list(indexed_evidences.values())\n",
    "w2v_model = train_word2vec(all_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_claims = prepare_indexed_text(train_claims, word2idx)\n",
    "indexed_evidences = {id: prepare_indexed_text({id: text}, word2idx)[0] for id, text in evidences.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indexed_evidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换claims和evidences为向量\n",
    "claim_vectors = {claim_id: text_to_vector(claim, w2v_model) for claim_id, claim in zip(train_claims.keys(), indexed_claims)}\n",
    "evidence_vectors = {id: text_to_vector(evidence, w2v_model) for id, evidence in indexed_evidences.items()}\n",
    "\n",
    "# 计算相似度并选择最相关的evidence\n",
    "top_evidences = {}\n",
    "for claim_id, claim_vector in claim_vectors.items():\n",
    "    similarities = {evidence_id: cosine_similarity([claim_vector], [evidence_vector])[0][0] for evidence_id, evidence_vector in evidence_vectors.items()}\n",
    "    sorted_evidences = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_evidences[claim_id] = sorted_evidences[:5]  # 选择相似度最高的5个evidence\n",
    "\n",
    "print(top_evidences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
